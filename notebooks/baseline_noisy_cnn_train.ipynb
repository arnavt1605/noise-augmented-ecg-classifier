{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13452833,"sourceType":"datasetVersion","datasetId":8539274},{"sourceId":13453138,"sourceType":"datasetVersion","datasetId":8539487}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:04:19.128115Z","iopub.execute_input":"2025-10-25T16:04:19.128388Z","iopub.status.idle":"2025-10-25T16:04:19.133986Z","shell.execute_reply.started":"2025-10-25T16:04:19.128371Z","shell.execute_reply":"2025-10-25T16:04:19.133188Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"class ECGCNN(nn.Module):\n    def __init__(self, num_classes=5):\n        super(ECGCNN, self).__init__()\n        self.conv1 = nn.Conv1d(1, 32, kernel_size=5, stride=1, padding=2)\n        self.bn1 = nn.BatchNorm1d(32)\n        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n        self.bn3 = nn.BatchNorm1d(128)\n        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        # Assuming beat length 216 -> 108 -> 54 -> 27 after pooling\n        self.fc1 = nn.Linear(128 * 27, 256)\n        self.dropout = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:04:19.144723Z","iopub.execute_input":"2025-10-25T16:04:19.145058Z","iopub.status.idle":"2025-10-25T16:04:19.151685Z","shell.execute_reply.started":"2025-10-25T16:04:19.145043Z","shell.execute_reply":"2025-10-25T16:04:19.150993Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"clean_data_path = '/kaggle/input/ecg-datasets/ecg_clean.csv'\ndf = pd.read_csv(clean_data_path)\nprint(f\"Dataset loaded: {df.shape}\")\n\nif df['label'].dtype == 'object':\n    le = LabelEncoder()\n    df['label'] = le.fit_transform(df['label'])\n\nfeature_cols = [c for c in df.columns if c != 'label']\nfor c in feature_cols:\n    df[c] = pd.to_numeric(df[c], errors='coerce')\ndf = df.dropna()\n\nX = torch.tensor(df[feature_cols].values, dtype=torch.float32)\ny = torch.tensor(df['label'].values, dtype=torch.long)\n\n# Standardize per feature\nX = (X - X.mean(0)) / (X.std(0) + 1e-8)\n\nprint(f\"X shape: {X.shape}, y shape: {y.shape}\")\nprint(\"Class dist:\", np.bincount(y.numpy()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:04:19.162812Z","iopub.execute_input":"2025-10-25T16:04:19.163016Z","iopub.status.idle":"2025-10-25T16:04:25.088248Z","shell.execute_reply.started":"2025-10-25T16:04:19.163002Z","shell.execute_reply":"2025-10-25T16:04:25.087582Z"}},"outputs":[{"name":"stdout","text":"Dataset loaded: (100033, 217)\nX shape: torch.Size([100033, 216]), y shape: torch.Size([100033])\nClass dist: [ 2546  8073 75028  7257  7129]\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"def add_awgn(signal, snr_db):\n    signal_power = torch.mean(signal ** 2)\n    noise_power = signal_power / (10 ** (snr_db / 10))\n    noise = torch.randn_like(signal) * torch.sqrt(noise_power)\n    return signal + noise\n\nclass NoisyDataset(torch.utils.data.Dataset):\n    def __init__(self, X, y, add_noise_prob=0.7, snr_levels=(0,3,6,9,12,15,18,20)):\n        self.X = X\n        self.y = y\n        self.add_noise_prob = add_noise_prob\n        self.snr_levels = snr_levels\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        x = self.X[idx]\n        y = self.y[idx]\n        if torch.rand(1).item() < self.add_noise_prob:\n            snr = float(np.random.choice(self.snr_levels))\n            x = add_awgn(x, snr)\n        return x, y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:04:25.089508Z","iopub.execute_input":"2025-10-25T16:04:25.089709Z","iopub.status.idle":"2025-10-25T16:04:25.095685Z","shell.execute_reply.started":"2025-10-25T16:04:25.089694Z","shell.execute_reply":"2025-10-25T16:04:25.094937Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def evaluate_with_metrics(model, loader, device):\n    model.eval()\n    preds, labels = [], []\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            logits = model(xb)\n            pred = torch.argmax(logits, 1)\n            preds.append(pred.cpu().numpy())\n            labels.append(yb.cpu().numpy())\n    preds = np.concatenate(preds)\n    labels = np.concatenate(labels)\n    acc = (preds == labels).mean()\n    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n    return {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1_score': f1}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:04:25.096501Z","iopub.execute_input":"2025-10-25T16:04:25.096742Z","iopub.status.idle":"2025-10-25T16:04:25.108385Z","shell.execute_reply.started":"2025-10-25T16:04:25.096724Z","shell.execute_reply":"2025-10-25T16:04:25.107766Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def train_one_fold(train_loader, val_loader, device, num_epochs=30, patience=10):\n    model = ECGCNN(num_classes=5).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    best_val_f1 = -1.0\n    best_state = None\n    patience_ctr = 0\n    history = {'train_loss': [], 'train_acc': [], 'val_acc': [], 'val_f1': []}\n\n    for epoch in range(num_epochs):\n        model.train()\n        total, correct, loss_sum = 0, 0, 0.0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            optimizer.step()\n            loss_sum += loss.item()\n            pred = torch.argmax(logits, 1)\n            correct += (pred == yb).sum().item()\n            total += yb.size(0)\n\n        train_acc = correct / total\n        history['train_loss'].append(loss_sum / max(1, len(train_loader)))\n        history['train_acc'].append(train_acc)\n\n        # Validation\n        val_metrics = evaluate_with_metrics(model, val_loader, device)\n        history['val_acc'].append(val_metrics['accuracy'])\n        history['val_f1'].append(val_metrics['f1_score'])\n\n        print(f\"  Epoch {epoch+1:2d}/30 - \"\n              f\"Loss: {history['train_loss'][-1]:.4f} - \"\n              f\"Train Acc: {train_acc:.4f} - \"\n              f\"Val Acc: {val_metrics['accuracy']:.4f} - \"\n              f\"Val F1: {val_metrics['f1_score']:.4f}\")\n\n        if val_metrics['f1_score'] > best_val_f1:\n            best_val_f1 = val_metrics['f1_score']\n            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n            patience_ctr = 0\n        else:\n            patience_ctr += 1\n            if patience_ctr >= patience:\n                print(f\"  ✓ Early stopping at epoch {epoch+1}\")\n                break\n\n    model.load_state_dict(best_state)\n    model = model.to(device)\n    return model, history, best_val_f1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:04:25.109127Z","iopub.execute_input":"2025-10-25T16:04:25.109411Z","iopub.status.idle":"2025-10-25T16:04:25.123284Z","shell.execute_reply.started":"2025-10-25T16:04:25.109389Z","shell.execute_reply":"2025-10-25T16:04:25.122604Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def train_noise_aug_with_kfold(X, y, k=5, num_epochs=30, patience=10, batch_size=64, add_noise_prob=0.7):\n    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n    fold_results, histories = [], []\n\n    print(\"\\n\" + \"=\"*70)\n    print(f\"NOISE-AUGMENTED CNN TRAINING WITH {k}-FOLD CV\")\n    print(\"=\"*70 + \"\\n\")\n\n    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y)):\n        print(\"\\n\" + \"=\"*70)\n        print(f\"FOLD {fold+1}/{k}\")\n        print(\"=\"*70)\n\n        X_tr, X_va = X[tr_idx], X[va_idx]\n        y_tr, y_va = y[tr_idx], y[va_idx]\n\n        train_ds = NoisyDataset(X_tr, y_tr, add_noise_prob=add_noise_prob)\n        val_ds   = TensorDataset(X_va, y_va)\n\n        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n        val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n\n        model, hist, best_f1 = train_one_fold(train_loader, val_loader, device, num_epochs, patience)\n        val_metrics = evaluate_with_metrics(model, val_loader, device)\n\n        print(f\"\\nFold {fold+1} Results:\")\n        print(f\"  Accuracy:  {val_metrics['accuracy']:.4f}\")\n        print(f\"  Precision: {val_metrics['precision']:.4f}\")\n        print(f\"  Recall:    {val_metrics['recall']:.4f}\")\n        print(f\"  F1-Score:  {val_metrics['f1_score']:.4f}\")\n\n        fold_results.append({\n            'fold': fold+1,\n            'accuracy': val_metrics['accuracy'],\n            'precision': val_metrics['precision'],\n            'recall': val_metrics['recall'],\n            'f1_score': val_metrics['f1_score'],\n        })\n        histories.append(hist)\n\n        torch.save(model.state_dict(), f'/kaggle/working/aug_model_fold_{fold+1}.pth')\n\n    avg = {\n        'accuracy_mean': float(np.mean([r['accuracy'] for r in fold_results])),\n        'accuracy_std': float(np.std([r['accuracy'] for r in fold_results])),\n        'precision_mean': float(np.mean([r['precision'] for r in fold_results])),\n        'precision_std': float(np.std([r['precision'] for r in fold_results])),\n        'recall_mean': float(np.mean([r['recall'] for r in fold_results])),\n        'recall_std': float(np.std([r['recall'] for r in fold_results])),\n        'f1_score_mean': float(np.mean([r['f1_score'] for r in fold_results])),\n        'f1_score_std': float(np.std([r['f1_score'] for r in fold_results])),\n    }\n\n    pd.DataFrame(fold_results).to_csv('/kaggle/working/kfold_results.csv', index=False)\n    with open('/kaggle/working/avg_metrics.json', 'w') as f:\n        json.dump(avg, f, indent=2)\n\n    print(\"\\n\" + \"=\"*70)\n    print(\"CROSS-VALIDATION SUMMARY (NOISE-AUGMENTED)\")\n    print(\"=\"*70)\n    print(f\"Accuracy:  {avg['accuracy_mean']:.4f} ± {avg['accuracy_std']:.4f}\")\n    print(f\"Precision: {avg['precision_mean']:.4f} ± {avg['precision_std']:.4f}\")\n    print(f\"Recall:    {avg['recall_mean']:.4f} ± {avg['recall_std']:.4f}\")\n    print(f\"F1-Score:  {avg['f1_score_mean']:.4f} ± {avg['f1_score_std']:.4f}\")\n    print(\"=\"*70 + \"\\n\")\n\n    return fold_results, avg, histories\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:04:25.124851Z","iopub.execute_input":"2025-10-25T16:04:25.125354Z","iopub.status.idle":"2025-10-25T16:04:25.140760Z","shell.execute_reply.started":"2025-10-25T16:04:25.125337Z","shell.execute_reply":"2025-10-25T16:04:25.140030Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"fold_results, avg_metrics, histories = train_noise_aug_with_kfold(\n    X, y, \n    k=5,\n    num_epochs=30,\n    patience=10,\n    batch_size=64,\n    add_noise_prob=0.7\n)\n\nprint(\" Noise-augmented k-fold training complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:04:25.141501Z","iopub.execute_input":"2025-10-25T16:04:25.141738Z","iopub.status.idle":"2025-10-25T17:44:10.041722Z","shell.execute_reply.started":"2025-10-25T16:04:25.141723Z","shell.execute_reply":"2025-10-25T17:44:10.040918Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nNOISE-AUGMENTED CNN TRAINING WITH 5-FOLD CV\n======================================================================\n\n\n======================================================================\nFOLD 1/5\n======================================================================\n  Epoch  1/30 - Loss: 0.1269 - Train Acc: 0.9636 - Val Acc: 0.9866 - Val F1: 0.9861\n  Epoch  2/30 - Loss: 0.0701 - Train Acc: 0.9795 - Val Acc: 0.9886 - Val F1: 0.9883\n  Epoch  3/30 - Loss: 0.0587 - Train Acc: 0.9822 - Val Acc: 0.9898 - Val F1: 0.9895\n  Epoch  4/30 - Loss: 0.0527 - Train Acc: 0.9836 - Val Acc: 0.9901 - Val F1: 0.9899\n  Epoch  5/30 - Loss: 0.0474 - Train Acc: 0.9853 - Val Acc: 0.9911 - Val F1: 0.9909\n  Epoch  6/30 - Loss: 0.0432 - Train Acc: 0.9875 - Val Acc: 0.9927 - Val F1: 0.9926\n  Epoch  7/30 - Loss: 0.0391 - Train Acc: 0.9883 - Val Acc: 0.9922 - Val F1: 0.9922\n  Epoch  8/30 - Loss: 0.0384 - Train Acc: 0.9881 - Val Acc: 0.9924 - Val F1: 0.9923\n  Epoch  9/30 - Loss: 0.0357 - Train Acc: 0.9891 - Val Acc: 0.9922 - Val F1: 0.9921\n  Epoch 10/30 - Loss: 0.0352 - Train Acc: 0.9893 - Val Acc: 0.9936 - Val F1: 0.9935\n  Epoch 11/30 - Loss: 0.0323 - Train Acc: 0.9898 - Val Acc: 0.9936 - Val F1: 0.9935\n  Epoch 12/30 - Loss: 0.0305 - Train Acc: 0.9904 - Val Acc: 0.9937 - Val F1: 0.9936\n  Epoch 13/30 - Loss: 0.0304 - Train Acc: 0.9906 - Val Acc: 0.9933 - Val F1: 0.9933\n  Epoch 14/30 - Loss: 0.0298 - Train Acc: 0.9906 - Val Acc: 0.9938 - Val F1: 0.9938\n  Epoch 15/30 - Loss: 0.0268 - Train Acc: 0.9913 - Val Acc: 0.9935 - Val F1: 0.9934\n  Epoch 16/30 - Loss: 0.0267 - Train Acc: 0.9915 - Val Acc: 0.9934 - Val F1: 0.9932\n  Epoch 17/30 - Loss: 0.0247 - Train Acc: 0.9919 - Val Acc: 0.9938 - Val F1: 0.9937\n  Epoch 18/30 - Loss: 0.0260 - Train Acc: 0.9917 - Val Acc: 0.9941 - Val F1: 0.9940\n  Epoch 19/30 - Loss: 0.0244 - Train Acc: 0.9925 - Val Acc: 0.9937 - Val F1: 0.9936\n  Epoch 20/30 - Loss: 0.0237 - Train Acc: 0.9924 - Val Acc: 0.9939 - Val F1: 0.9938\n  Epoch 21/30 - Loss: 0.0231 - Train Acc: 0.9927 - Val Acc: 0.9936 - Val F1: 0.9936\n  Epoch 22/30 - Loss: 0.0226 - Train Acc: 0.9928 - Val Acc: 0.9935 - Val F1: 0.9935\n  Epoch 23/30 - Loss: 0.0209 - Train Acc: 0.9932 - Val Acc: 0.9941 - Val F1: 0.9940\n  Epoch 24/30 - Loss: 0.0205 - Train Acc: 0.9936 - Val Acc: 0.9937 - Val F1: 0.9936\n  Epoch 25/30 - Loss: 0.0204 - Train Acc: 0.9932 - Val Acc: 0.9939 - Val F1: 0.9938\n  Epoch 26/30 - Loss: 0.0208 - Train Acc: 0.9935 - Val Acc: 0.9941 - Val F1: 0.9940\n  Epoch 27/30 - Loss: 0.0196 - Train Acc: 0.9936 - Val Acc: 0.9937 - Val F1: 0.9937\n  Epoch 28/30 - Loss: 0.0193 - Train Acc: 0.9936 - Val Acc: 0.9942 - Val F1: 0.9941\n  Epoch 29/30 - Loss: 0.0180 - Train Acc: 0.9940 - Val Acc: 0.9943 - Val F1: 0.9942\n  Epoch 30/30 - Loss: 0.0198 - Train Acc: 0.9936 - Val Acc: 0.9941 - Val F1: 0.9940\n\nFold 1 Results:\n  Accuracy:  0.9943\n  Precision: 0.9942\n  Recall:    0.9943\n  F1-Score:  0.9942\n\n======================================================================\nFOLD 2/5\n======================================================================\n  Epoch  1/30 - Loss: 0.1214 - Train Acc: 0.9655 - Val Acc: 0.9861 - Val F1: 0.9856\n  Epoch  2/30 - Loss: 0.0681 - Train Acc: 0.9799 - Val Acc: 0.9853 - Val F1: 0.9846\n  Epoch  3/30 - Loss: 0.0577 - Train Acc: 0.9830 - Val Acc: 0.9895 - Val F1: 0.9891\n  Epoch  4/30 - Loss: 0.0512 - Train Acc: 0.9847 - Val Acc: 0.9889 - Val F1: 0.9884\n  Epoch  5/30 - Loss: 0.0470 - Train Acc: 0.9862 - Val Acc: 0.9899 - Val F1: 0.9895\n  Epoch  6/30 - Loss: 0.0430 - Train Acc: 0.9874 - Val Acc: 0.9910 - Val F1: 0.9908\n  Epoch  7/30 - Loss: 0.0394 - Train Acc: 0.9878 - Val Acc: 0.9922 - Val F1: 0.9920\n  Epoch  8/30 - Loss: 0.0359 - Train Acc: 0.9891 - Val Acc: 0.9918 - Val F1: 0.9916\n  Epoch  9/30 - Loss: 0.0361 - Train Acc: 0.9889 - Val Acc: 0.9918 - Val F1: 0.9916\n  Epoch 10/30 - Loss: 0.0310 - Train Acc: 0.9905 - Val Acc: 0.9915 - Val F1: 0.9913\n  Epoch 11/30 - Loss: 0.0302 - Train Acc: 0.9907 - Val Acc: 0.9908 - Val F1: 0.9905\n  Epoch 12/30 - Loss: 0.0296 - Train Acc: 0.9907 - Val Acc: 0.9930 - Val F1: 0.9928\n  Epoch 13/30 - Loss: 0.0293 - Train Acc: 0.9909 - Val Acc: 0.9929 - Val F1: 0.9927\n  Epoch 14/30 - Loss: 0.0263 - Train Acc: 0.9920 - Val Acc: 0.9931 - Val F1: 0.9929\n  Epoch 15/30 - Loss: 0.0262 - Train Acc: 0.9921 - Val Acc: 0.9932 - Val F1: 0.9930\n  Epoch 16/30 - Loss: 0.0254 - Train Acc: 0.9916 - Val Acc: 0.9934 - Val F1: 0.9933\n  Epoch 17/30 - Loss: 0.0256 - Train Acc: 0.9920 - Val Acc: 0.9931 - Val F1: 0.9929\n  Epoch 18/30 - Loss: 0.0236 - Train Acc: 0.9927 - Val Acc: 0.9931 - Val F1: 0.9929\n  Epoch 19/30 - Loss: 0.0245 - Train Acc: 0.9922 - Val Acc: 0.9936 - Val F1: 0.9935\n  Epoch 20/30 - Loss: 0.0228 - Train Acc: 0.9925 - Val Acc: 0.9932 - Val F1: 0.9931\n  Epoch 21/30 - Loss: 0.0224 - Train Acc: 0.9926 - Val Acc: 0.9936 - Val F1: 0.9935\n  Epoch 22/30 - Loss: 0.0215 - Train Acc: 0.9931 - Val Acc: 0.9934 - Val F1: 0.9933\n  Epoch 23/30 - Loss: 0.0214 - Train Acc: 0.9934 - Val Acc: 0.9936 - Val F1: 0.9935\n  Epoch 24/30 - Loss: 0.0219 - Train Acc: 0.9930 - Val Acc: 0.9937 - Val F1: 0.9935\n  Epoch 25/30 - Loss: 0.0202 - Train Acc: 0.9935 - Val Acc: 0.9939 - Val F1: 0.9938\n  Epoch 26/30 - Loss: 0.0204 - Train Acc: 0.9936 - Val Acc: 0.9935 - Val F1: 0.9934\n  Epoch 27/30 - Loss: 0.0191 - Train Acc: 0.9937 - Val Acc: 0.9936 - Val F1: 0.9935\n  Epoch 28/30 - Loss: 0.0188 - Train Acc: 0.9940 - Val Acc: 0.9932 - Val F1: 0.9931\n  Epoch 29/30 - Loss: 0.0190 - Train Acc: 0.9939 - Val Acc: 0.9935 - Val F1: 0.9934\n  Epoch 30/30 - Loss: 0.0185 - Train Acc: 0.9942 - Val Acc: 0.9933 - Val F1: 0.9932\n\nFold 2 Results:\n  Accuracy:  0.9939\n  Precision: 0.9938\n  Recall:    0.9939\n  F1-Score:  0.9938\n\n======================================================================\nFOLD 3/5\n======================================================================\n  Epoch  1/30 - Loss: 0.1176 - Train Acc: 0.9665 - Val Acc: 0.9873 - Val F1: 0.9867\n  Epoch  2/30 - Loss: 0.0683 - Train Acc: 0.9805 - Val Acc: 0.9896 - Val F1: 0.9894\n  Epoch  3/30 - Loss: 0.0587 - Train Acc: 0.9828 - Val Acc: 0.9907 - Val F1: 0.9905\n  Epoch  4/30 - Loss: 0.0521 - Train Acc: 0.9840 - Val Acc: 0.9919 - Val F1: 0.9917\n  Epoch  5/30 - Loss: 0.0449 - Train Acc: 0.9869 - Val Acc: 0.9922 - Val F1: 0.9920\n  Epoch  6/30 - Loss: 0.0422 - Train Acc: 0.9870 - Val Acc: 0.9929 - Val F1: 0.9928\n  Epoch  7/30 - Loss: 0.0403 - Train Acc: 0.9880 - Val Acc: 0.9928 - Val F1: 0.9925\n  Epoch  8/30 - Loss: 0.0364 - Train Acc: 0.9888 - Val Acc: 0.9939 - Val F1: 0.9938\n  Epoch  9/30 - Loss: 0.0356 - Train Acc: 0.9892 - Val Acc: 0.9937 - Val F1: 0.9936\n  Epoch 10/30 - Loss: 0.0322 - Train Acc: 0.9902 - Val Acc: 0.9939 - Val F1: 0.9938\n  Epoch 11/30 - Loss: 0.0318 - Train Acc: 0.9906 - Val Acc: 0.9941 - Val F1: 0.9940\n  Epoch 12/30 - Loss: 0.0301 - Train Acc: 0.9906 - Val Acc: 0.9943 - Val F1: 0.9943\n  Epoch 13/30 - Loss: 0.0299 - Train Acc: 0.9910 - Val Acc: 0.9935 - Val F1: 0.9933\n  Epoch 14/30 - Loss: 0.0272 - Train Acc: 0.9917 - Val Acc: 0.9941 - Val F1: 0.9940\n  Epoch 15/30 - Loss: 0.0261 - Train Acc: 0.9916 - Val Acc: 0.9946 - Val F1: 0.9944\n  Epoch 16/30 - Loss: 0.0252 - Train Acc: 0.9923 - Val Acc: 0.9940 - Val F1: 0.9939\n  Epoch 17/30 - Loss: 0.0247 - Train Acc: 0.9917 - Val Acc: 0.9945 - Val F1: 0.9944\n  Epoch 18/30 - Loss: 0.0224 - Train Acc: 0.9927 - Val Acc: 0.9950 - Val F1: 0.9949\n  Epoch 19/30 - Loss: 0.0234 - Train Acc: 0.9922 - Val Acc: 0.9945 - Val F1: 0.9944\n  Epoch 20/30 - Loss: 0.0223 - Train Acc: 0.9925 - Val Acc: 0.9948 - Val F1: 0.9947\n  Epoch 21/30 - Loss: 0.0217 - Train Acc: 0.9923 - Val Acc: 0.9944 - Val F1: 0.9943\n  Epoch 22/30 - Loss: 0.0211 - Train Acc: 0.9936 - Val Acc: 0.9946 - Val F1: 0.9945\n  Epoch 23/30 - Loss: 0.0208 - Train Acc: 0.9933 - Val Acc: 0.9951 - Val F1: 0.9950\n  Epoch 24/30 - Loss: 0.0209 - Train Acc: 0.9933 - Val Acc: 0.9946 - Val F1: 0.9945\n  Epoch 25/30 - Loss: 0.0216 - Train Acc: 0.9931 - Val Acc: 0.9949 - Val F1: 0.9948\n  Epoch 26/30 - Loss: 0.0195 - Train Acc: 0.9935 - Val Acc: 0.9950 - Val F1: 0.9949\n  Epoch 27/30 - Loss: 0.0191 - Train Acc: 0.9939 - Val Acc: 0.9952 - Val F1: 0.9951\n  Epoch 28/30 - Loss: 0.0194 - Train Acc: 0.9938 - Val Acc: 0.9950 - Val F1: 0.9949\n  Epoch 29/30 - Loss: 0.0185 - Train Acc: 0.9941 - Val Acc: 0.9948 - Val F1: 0.9947\n  Epoch 30/30 - Loss: 0.0181 - Train Acc: 0.9942 - Val Acc: 0.9951 - Val F1: 0.9950\n\nFold 3 Results:\n  Accuracy:  0.9952\n  Precision: 0.9952\n  Recall:    0.9952\n  F1-Score:  0.9951\n\n======================================================================\nFOLD 4/5\n======================================================================\n  Epoch  1/30 - Loss: 0.1195 - Train Acc: 0.9670 - Val Acc: 0.9871 - Val F1: 0.9868\n  Epoch  2/30 - Loss: 0.0682 - Train Acc: 0.9802 - Val Acc: 0.9891 - Val F1: 0.9888\n  Epoch  3/30 - Loss: 0.0565 - Train Acc: 0.9835 - Val Acc: 0.9895 - Val F1: 0.9893\n  Epoch  4/30 - Loss: 0.0486 - Train Acc: 0.9857 - Val Acc: 0.9907 - Val F1: 0.9906\n  Epoch  5/30 - Loss: 0.0455 - Train Acc: 0.9863 - Val Acc: 0.9907 - Val F1: 0.9905\n  Epoch  6/30 - Loss: 0.0402 - Train Acc: 0.9877 - Val Acc: 0.9911 - Val F1: 0.9908\n  Epoch  7/30 - Loss: 0.0376 - Train Acc: 0.9886 - Val Acc: 0.9920 - Val F1: 0.9919\n  Epoch  8/30 - Loss: 0.0357 - Train Acc: 0.9889 - Val Acc: 0.9928 - Val F1: 0.9927\n  Epoch  9/30 - Loss: 0.0327 - Train Acc: 0.9903 - Val Acc: 0.9926 - Val F1: 0.9925\n  Epoch 10/30 - Loss: 0.0305 - Train Acc: 0.9905 - Val Acc: 0.9931 - Val F1: 0.9930\n  Epoch 11/30 - Loss: 0.0298 - Train Acc: 0.9912 - Val Acc: 0.9925 - Val F1: 0.9925\n  Epoch 12/30 - Loss: 0.0279 - Train Acc: 0.9911 - Val Acc: 0.9938 - Val F1: 0.9938\n  Epoch 13/30 - Loss: 0.0268 - Train Acc: 0.9917 - Val Acc: 0.9933 - Val F1: 0.9932\n  Epoch 14/30 - Loss: 0.0260 - Train Acc: 0.9914 - Val Acc: 0.9937 - Val F1: 0.9936\n  Epoch 15/30 - Loss: 0.0238 - Train Acc: 0.9924 - Val Acc: 0.9939 - Val F1: 0.9939\n  Epoch 16/30 - Loss: 0.0237 - Train Acc: 0.9925 - Val Acc: 0.9939 - Val F1: 0.9939\n  Epoch 17/30 - Loss: 0.0236 - Train Acc: 0.9928 - Val Acc: 0.9931 - Val F1: 0.9930\n  Epoch 18/30 - Loss: 0.0232 - Train Acc: 0.9926 - Val Acc: 0.9939 - Val F1: 0.9938\n  Epoch 19/30 - Loss: 0.0217 - Train Acc: 0.9931 - Val Acc: 0.9939 - Val F1: 0.9939\n  Epoch 20/30 - Loss: 0.0213 - Train Acc: 0.9933 - Val Acc: 0.9937 - Val F1: 0.9936\n  Epoch 21/30 - Loss: 0.0207 - Train Acc: 0.9934 - Val Acc: 0.9939 - Val F1: 0.9939\n  Epoch 22/30 - Loss: 0.0200 - Train Acc: 0.9936 - Val Acc: 0.9944 - Val F1: 0.9943\n  Epoch 23/30 - Loss: 0.0197 - Train Acc: 0.9936 - Val Acc: 0.9942 - Val F1: 0.9941\n  Epoch 24/30 - Loss: 0.0190 - Train Acc: 0.9937 - Val Acc: 0.9941 - Val F1: 0.9940\n  Epoch 25/30 - Loss: 0.0188 - Train Acc: 0.9941 - Val Acc: 0.9940 - Val F1: 0.9939\n  Epoch 26/30 - Loss: 0.0181 - Train Acc: 0.9943 - Val Acc: 0.9945 - Val F1: 0.9944\n  Epoch 27/30 - Loss: 0.0175 - Train Acc: 0.9941 - Val Acc: 0.9946 - Val F1: 0.9945\n  Epoch 28/30 - Loss: 0.0177 - Train Acc: 0.9939 - Val Acc: 0.9946 - Val F1: 0.9946\n  Epoch 29/30 - Loss: 0.0179 - Train Acc: 0.9945 - Val Acc: 0.9941 - Val F1: 0.9940\n  Epoch 30/30 - Loss: 0.0169 - Train Acc: 0.9943 - Val Acc: 0.9947 - Val F1: 0.9946\n\nFold 4 Results:\n  Accuracy:  0.9947\n  Precision: 0.9946\n  Recall:    0.9947\n  F1-Score:  0.9946\n\n======================================================================\nFOLD 5/5\n======================================================================\n  Epoch  1/30 - Loss: 0.1208 - Train Acc: 0.9655 - Val Acc: 0.9845 - Val F1: 0.9839\n  Epoch  2/30 - Loss: 0.0698 - Train Acc: 0.9799 - Val Acc: 0.9884 - Val F1: 0.9882\n  Epoch  3/30 - Loss: 0.0569 - Train Acc: 0.9827 - Val Acc: 0.9890 - Val F1: 0.9886\n  Epoch  4/30 - Loss: 0.0496 - Train Acc: 0.9852 - Val Acc: 0.9903 - Val F1: 0.9901\n  Epoch  5/30 - Loss: 0.0452 - Train Acc: 0.9867 - Val Acc: 0.9915 - Val F1: 0.9913\n  Epoch  6/30 - Loss: 0.0413 - Train Acc: 0.9877 - Val Acc: 0.9911 - Val F1: 0.9910\n  Epoch  7/30 - Loss: 0.0398 - Train Acc: 0.9881 - Val Acc: 0.9910 - Val F1: 0.9908\n  Epoch  8/30 - Loss: 0.0369 - Train Acc: 0.9888 - Val Acc: 0.9918 - Val F1: 0.9917\n  Epoch  9/30 - Loss: 0.0345 - Train Acc: 0.9892 - Val Acc: 0.9924 - Val F1: 0.9923\n  Epoch 10/30 - Loss: 0.0329 - Train Acc: 0.9901 - Val Acc: 0.9926 - Val F1: 0.9926\n  Epoch 11/30 - Loss: 0.0302 - Train Acc: 0.9906 - Val Acc: 0.9923 - Val F1: 0.9923\n  Epoch 12/30 - Loss: 0.0302 - Train Acc: 0.9909 - Val Acc: 0.9928 - Val F1: 0.9927\n  Epoch 13/30 - Loss: 0.0274 - Train Acc: 0.9914 - Val Acc: 0.9936 - Val F1: 0.9935\n  Epoch 14/30 - Loss: 0.0275 - Train Acc: 0.9920 - Val Acc: 0.9930 - Val F1: 0.9929\n  Epoch 15/30 - Loss: 0.0251 - Train Acc: 0.9920 - Val Acc: 0.9930 - Val F1: 0.9929\n  Epoch 16/30 - Loss: 0.0237 - Train Acc: 0.9924 - Val Acc: 0.9936 - Val F1: 0.9935\n  Epoch 17/30 - Loss: 0.0246 - Train Acc: 0.9922 - Val Acc: 0.9938 - Val F1: 0.9937\n  Epoch 18/30 - Loss: 0.0238 - Train Acc: 0.9927 - Val Acc: 0.9933 - Val F1: 0.9932\n  Epoch 19/30 - Loss: 0.0235 - Train Acc: 0.9928 - Val Acc: 0.9938 - Val F1: 0.9937\n  Epoch 20/30 - Loss: 0.0239 - Train Acc: 0.9929 - Val Acc: 0.9939 - Val F1: 0.9938\n  Epoch 21/30 - Loss: 0.0226 - Train Acc: 0.9927 - Val Acc: 0.9943 - Val F1: 0.9942\n  Epoch 22/30 - Loss: 0.0200 - Train Acc: 0.9935 - Val Acc: 0.9945 - Val F1: 0.9944\n  Epoch 23/30 - Loss: 0.0198 - Train Acc: 0.9936 - Val Acc: 0.9940 - Val F1: 0.9939\n  Epoch 24/30 - Loss: 0.0195 - Train Acc: 0.9937 - Val Acc: 0.9938 - Val F1: 0.9938\n  Epoch 25/30 - Loss: 0.0195 - Train Acc: 0.9934 - Val Acc: 0.9935 - Val F1: 0.9935\n  Epoch 26/30 - Loss: 0.0197 - Train Acc: 0.9935 - Val Acc: 0.9934 - Val F1: 0.9932\n  Epoch 27/30 - Loss: 0.0180 - Train Acc: 0.9942 - Val Acc: 0.9934 - Val F1: 0.9933\n  Epoch 28/30 - Loss: 0.0186 - Train Acc: 0.9940 - Val Acc: 0.9940 - Val F1: 0.9940\n  Epoch 29/30 - Loss: 0.0173 - Train Acc: 0.9947 - Val Acc: 0.9938 - Val F1: 0.9937\n  Epoch 30/30 - Loss: 0.0183 - Train Acc: 0.9943 - Val Acc: 0.9938 - Val F1: 0.9937\n\nFold 5 Results:\n  Accuracy:  0.9945\n  Precision: 0.9944\n  Recall:    0.9945\n  F1-Score:  0.9944\n\n======================================================================\nCROSS-VALIDATION SUMMARY (NOISE-AUGMENTED)\n======================================================================\nAccuracy:  0.9945 ± 0.0004\nPrecision: 0.9945 ± 0.0004\nRecall:    0.9945 ± 0.0004\nF1-Score:  0.9944 ± 0.0004\n======================================================================\n\n Noise-augmented k-fold training complete.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Select best fold by F1\nbest_fold_idx = int(np.argmax([r['f1_score'] for r in fold_results])) + 1\nprint(f\"Best fold (noise-aug): {best_fold_idx}\")\n\nbest_model = ECGCNN(num_classes=5).to(device)\nbest_model.load_state_dict(torch.load(f'/kaggle/working/aug_model_fold_{best_fold_idx}.pth'))\n\ndef evaluate_on_noisy_snr(model, X_clean, y_clean, snr, batch_size=64):\n    noisy = torch.stack([add_awgn(x, snr) for x in X_clean], dim=0)\n    loader = DataLoader(TensorDataset(noisy, y_clean), batch_size=batch_size, shuffle=False)\n    return evaluate_with_metrics(model, loader, device)\n\nsnr_levels = [0,3,6,9,12,15,18,20]\nrows = []\nfor snr in snr_levels:\n    m = evaluate_on_noisy_snr(best_model, X, y, snr)\n    rows.append({\n        'snr_db': int(snr),\n        'accuracy': float(m['accuracy']),\n        'precision': float(m['precision']),\n        'recall': float(m['recall']),\n        'f1_score': float(m['f1_score'])\n    })\n    print(f\"SNR {snr} dB -> acc={m['accuracy']:.4f}  f1={m['f1_score']:.4f}\")\n\nsnr_df = pd.DataFrame(rows)\nsnr_df.to_csv('/kaggle/working/snr_evaluation_results.csv', index=False)\nprint(\"✅ SNR evaluation saved to snr_evaluation_results.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T17:44:10.042659Z","iopub.execute_input":"2025-10-25T17:44:10.043294Z","iopub.status.idle":"2025-10-25T17:45:02.140529Z","shell.execute_reply.started":"2025-10-25T17:44:10.043269Z","shell.execute_reply":"2025-10-25T17:45:02.139907Z"}},"outputs":[{"name":"stdout","text":"Best fold (noise-aug): 3\nSNR 0 dB -> acc=0.9872  f1=0.9869\nSNR 3 dB -> acc=0.9936  f1=0.9935\nSNR 6 dB -> acc=0.9957  f1=0.9956\nSNR 9 dB -> acc=0.9968  f1=0.9968\nSNR 12 dB -> acc=0.9975  f1=0.9975\nSNR 15 dB -> acc=0.9978  f1=0.9978\nSNR 18 dB -> acc=0.9980  f1=0.9980\nSNR 20 dB -> acc=0.9980  f1=0.9980\n✅ SNR evaluation saved to snr_evaluation_results.csv\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Cell 10: Evaluate on Noisy Test Sets at Different SNR Levels\nsnr_levels = [0, 3, 6, 9, 12, 15, 18, 20]\nbaseline_snr_results = []\n\nprint(f\"\\n{'SNR (dB)':<10} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\nprint(\"-\"*70)\n\nfor snr in snr_levels:\n    # Check if pre-made noisy CSV exists\n    noisy_csv_path = f'/kaggle/input/ecg-datasets/ecg_noisy_{snr}db.csv'\n    \n    if os.path.exists(noisy_csv_path):\n        # Load pre-made noisy dataset\n        df_noisy = pd.read_csv(noisy_csv_path)\n        \n        if df_noisy['label'].dtype == 'object':\n            le = LabelEncoder()\n            df_noisy['label'] = le.fit_transform(df_noisy['label'])\n        \n        feature_cols = [col for col in df_noisy.columns if col != 'label']\n        for col in feature_cols:\n            df_noisy[col] = pd.to_numeric(df_noisy[col], errors='coerce')\n        df_noisy = df_noisy.dropna()\n        \n        X_noisy = torch.tensor(df_noisy.drop('label', axis=1).values, dtype=torch.float32)\n        y_noisy = torch.tensor(df_noisy['label'].values, dtype=torch.long)\n        X_noisy = (X_noisy - X_noisy.mean(dim=0)) / (X_noisy.std(dim=0) + 1e-8)\n    else:\n        # Generate noisy data on-the-fly from clean data\n        X_noisy = torch.stack([add_awgn(x, snr) for x in X], dim=0)\n        y_noisy = y.clone()\n    \n    # Create loader\n    noisy_dataset = TensorDataset(X_noisy, y_noisy)\n    noisy_loader = DataLoader(noisy_dataset, batch_size=64, shuffle=False)\n    \n    # Evaluate\n    metrics = evaluate_with_metrics(best_baseline_model, noisy_loader, device)\n    \n    baseline_snr_results.append({\n        'snr_db': snr,\n        'baseline_accuracy': metrics['accuracy'],\n        'baseline_precision': metrics['precision'],\n        'baseline_recall': metrics['recall'],\n        'baseline_f1_score': metrics['f1_score']\n    })\n    \n    print(f\"{snr:<10} {metrics['accuracy']:<12.4f} {metrics['precision']:<12.4f} \"\n          f\"{metrics['recall']:<12.4f} {metrics['f1_score']:<12.4f}\")\n\nprint(\"-\"*70)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T17:45:02.141255Z","iopub.execute_input":"2025-10-25T17:45:02.141472Z","iopub.status.idle":"2025-10-25T17:46:04.530967Z","shell.execute_reply.started":"2025-10-25T17:45:02.141448Z","shell.execute_reply":"2025-10-25T17:46:04.530148Z"}},"outputs":[{"name":"stdout","text":"\nSNR (dB)   Accuracy     Precision    Recall       F1-Score    \n----------------------------------------------------------------------\n0          0.7537       0.7252       0.7537       0.7064      \n3          0.8014       0.8001       0.8014       0.7814      \n6          0.8638       0.8731       0.8638       0.8608      \n9          0.9135       0.9240       0.9135       0.9157      \n12         0.9407       0.9508       0.9407       0.9438      \n15         0.9577       0.9673       0.9577       0.9611      \n18         0.9738       0.9806       0.9738       0.9761      \n20         0.9836       0.9866       0.9836       0.9846      \n----------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Cell 11: Save Baseline SNR Results (FIXED)\nbaseline_snr_df = pd.DataFrame(baseline_snr_results)\nbaseline_snr_df.to_csv('/kaggle/working/baseline_snr_results.csv', index=False)\n\n# Calculate and save summary statistics\nsummary_stats = {\n    'model': 'Baseline CNN (Clean Data Only - K-Fold CV)',\n    'k_folds': 5,\n    'best_fold': int(best_fold),  # ← Convert to Python int\n    'snr_levels': [int(x) for x in snr_levels],  # ← Convert to Python int list\n    'clean_data_performance': {\n        'accuracy_mean': float(avg_metrics['accuracy_mean']),  # ← Convert to Python float\n        'accuracy_std': float(avg_metrics['accuracy_std']),\n        'f1_score_mean': float(avg_metrics['f1_score_mean']),\n        'f1_score_std': float(avg_metrics['f1_score_std'])\n    },\n    'noisy_data_performance': {\n        'avg_accuracy': float(baseline_snr_df['baseline_accuracy'].mean()),\n        'avg_precision': float(baseline_snr_df['baseline_precision'].mean()),\n        'avg_recall': float(baseline_snr_df['baseline_recall'].mean()),\n        'avg_f1_score': float(baseline_snr_df['baseline_f1_score'].mean()),\n        'worst_accuracy_snr': int(baseline_snr_df.loc[baseline_snr_df['baseline_accuracy'].idxmin(), 'snr_db']),\n        'worst_accuracy_value': float(baseline_snr_df['baseline_accuracy'].min()),\n        'best_accuracy_snr': int(baseline_snr_df.loc[baseline_snr_df['baseline_accuracy'].idxmax(), 'snr_db']),\n        'best_accuracy_value': float(baseline_snr_df['baseline_accuracy'].max())\n    }\n}\n\nwith open('/kaggle/working/baseline_summary.json', 'w') as f:\n    json.dump(summary_stats, f, indent=2)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T17:57:42.580363Z","iopub.execute_input":"2025-10-25T17:57:42.580633Z","iopub.status.idle":"2025-10-25T17:57:42.590260Z","shell.execute_reply.started":"2025-10-25T17:57:42.580614Z","shell.execute_reply":"2025-10-25T17:57:42.589559Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Cell 12: Display Complete Summary\nprint(\"\\n\" + \"=\"*70)\nprint(\"COMPLETE BASELINE CNN SUMMARY\")\nprint(\"=\"*70)\n\nprint(\"\\n1. K-Fold Cross-Validation Results (Clean Data):\")\nprint(f\"   Accuracy:  {avg_metrics['accuracy_mean']:.4f} ± {avg_metrics['accuracy_std']:.4f}\")\nprint(f\"   Precision: {avg_metrics['precision_mean']:.4f} ± {avg_metrics['precision_std']:.4f}\")\nprint(f\"   Recall:    {avg_metrics['recall_mean']:.4f} ± {avg_metrics['recall_std']:.4f}\")\nprint(f\"   F1-Score:  {avg_metrics['f1_score_mean']:.4f} ± {avg_metrics['f1_score_std']:.4f}\")\n\nprint(\"\\n2. Average Performance on Noisy Data (0-20 dB SNR):\")\nprint(f\"   Accuracy:  {summary_stats['noisy_data_performance']['avg_accuracy']:.4f}\")\nprint(f\"   Precision: {summary_stats['noisy_data_performance']['avg_precision']:.4f}\")\nprint(f\"   Recall:    {summary_stats['noisy_data_performance']['avg_recall']:.4f}\")\nprint(f\"   F1-Score:  {summary_stats['noisy_data_performance']['avg_f1_score']:.4f}\")\n\nprint(\"\\n3. Performance Range on Noisy Data:\")\nprint(f\"   Best:  {summary_stats['noisy_data_performance']['best_accuracy_value']:.4f} at SNR {summary_stats['noisy_data_performance']['best_accuracy_snr']} dB\")\nprint(f\"   Worst: {summary_stats['noisy_data_performance']['worst_accuracy_value']:.4f} at SNR {summary_stats['noisy_data_performance']['worst_accuracy_snr']} dB\")\nprint(f\"   Degradation: {(summary_stats['noisy_data_performance']['best_accuracy_value'] - summary_stats['noisy_data_performance']['worst_accuracy_value']) * 100:.2f}%\")\n\nprint(\"\\n4. Best Fold Model Used:\")\nprint(f\"   Fold: {best_fold}\")\nprint(f\"   F1-Score: {fold_results[best_fold_idx]['f1_score']:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T17:56:48.406407Z","iopub.execute_input":"2025-10-25T17:56:48.407082Z","iopub.status.idle":"2025-10-25T17:56:48.417512Z","shell.execute_reply.started":"2025-10-25T17:56:48.407056Z","shell.execute_reply":"2025-10-25T17:56:48.415935Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nCOMPLETE BASELINE CNN SUMMARY\n======================================================================\n\n1. K-Fold Cross-Validation Results (Clean Data):\n   Accuracy:  0.9945 ± 0.0004\n   Precision: 0.9945 ± 0.0004\n   Recall:    0.9945 ± 0.0004\n   F1-Score:  0.9944 ± 0.0004\n\n2. Average Performance on Noisy Data (0-20 dB SNR):\n   Accuracy:  0.8985\n   Precision: 0.9010\n   Recall:    0.8985\n   F1-Score:  0.8913\n\n3. Performance Range on Noisy Data:\n   Best:  0.9836 at SNR 20 dB\n   Worst: 0.7537 at SNR 0 dB\n   Degradation: 22.99%\n\n4. Best Fold Model Used:\n   Fold: 1\n   F1-Score: 0.9946\n","output_type":"stream"}],"execution_count":32}]}